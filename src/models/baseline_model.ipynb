{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c76bdeca",
   "metadata": {},
   "source": [
    "# Baseline multi-output classifier\n",
    "\n",
    "This notebook trains a simple baseline MultiOutput RandomForest classifier on the spectra dataset created by `src.preprocessing.loader`.\n",
    "It reports common multi-output/multi-label metrics and saves the trained model and a metrics JSON to `src/models/`.\n",
    "\n",
    "Notes:\n",
    "- Expects the repository root to include `src/` and the raw data under `data/raw/.../ATR set 1_washed`.\n",
    "- Uses spectra-based dataset (`prepare_ml_dataset_spectra`) from the loader module.\n",
    "- Adjust `TEST_SIZE` and random seeds as needed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72278a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: c:\\Users\\Mikey\\Documents\\GitHub\\cmse492_project\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'src.preprocessing.loader' from 'c:\\\\Users\\\\Mikey\\\\Documents\\\\GitHub\\\\cmse492_project\\\\src\\\\preprocessing\\\\loader.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports and repo-root discovery\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import json\n",
    "\n",
    "# find repo root (same approach as other notebooks)\n",
    "cwd = Path.cwd()\n",
    "repo_root = None\n",
    "for p in [cwd] + list(cwd.parents):\n",
    "    if (p / 'src').exists() or (p / '.git').exists():\n",
    "        repo_root = p\n",
    "        break\n",
    "if repo_root is None:\n",
    "    repo_root = cwd\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "print('Repo root:', repo_root)\n",
    "\n",
    "# ML imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, f1_score, classification_report, precision_recall_fscore_support\n",
    "import joblib\n",
    "\n",
    "# import loader and reload to pick up edits during development\n",
    "import src.preprocessing.loader as loader_mod\n",
    "importlib.reload(loader_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902e8e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder: c:\\Users\\Mikey\\Documents\\GitHub\\cmse492_project\\data\\raw\\Plastic Washing CMSE project CSV files\\ATR set 1_washed\n",
      "X shape: (122, 1868)\n",
      "y shape: (122, 9)\n",
      "X shape: (122, 1868)\n",
      "y shape: (122, 9)\n"
     ]
    }
   ],
   "source": [
    "# Load spectra-based dataset (X: n_samples x n_wavelengths, y: n_samples x n_targets)\n",
    "data_root = repo_root / 'data' / 'raw' / 'Plastic Washing CMSE project CSV files' / 'ATR set 1_washed'\n",
    "print('Data folder:', data_root)\n",
    "X, y, feature_names, target_names = loader_mod.prepare_ml_dataset_spectra(data_root)\n",
    "print('X shape:', X.shape if hasattr(X, 'shape') else None)\n",
    "print('y shape:', y.shape if hasattr(y, 'shape') else None)\n",
    "\n",
    "# Basic sanity checks\n",
    "if X.size == 0 or y.size == 0:\n",
    "    raise RuntimeError('No data loaded. Check data_root path and loader implementation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9cca42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test sizes: 97 25\n"
     ]
    }
   ],
   "source": [
    "# Prepare train/test split\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# attempt to stratify by polymer class (first 4 one-hot targets) when possible\n",
    "stratify_labels = None\n",
    "if y.shape[1] >= 4:\n",
    "    try:\n",
    "        stratify_labels = np.argmax(y[:, :4], axis=1)\n",
    "        # if stratify has at least two classes, use it, else fallback\n",
    "        if len(np.unique(stratify_labels)) < 2:\n",
    "            stratify_labels = None\n",
    "    except Exception:\n",
    "        stratify_labels = None\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=stratify_labels\n",
    ")\n",
    "print('Train/test sizes:', X_train.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95daa460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier...\n",
      "Done training\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train a MultiOutput RandomForest baseline\n",
    "clf = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=RANDOM_STATE))\n",
    "print('Fitting classifier...')\n",
    "clf.fit(X_train, y_train)\n",
    "print('Done training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f568f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact-match accuracy: 0.2\n",
      "Hamming loss: 0.21333333333333335\n",
      "Macro F1: 0.7238407760098764\n",
      "\n",
      "Per-target metrics:\n",
      "            precision    recall        f1  support\n",
      "is_HDPE      0.800000  0.800000  0.800000      5.0\n",
      "is_LDPE      1.000000  0.285714  0.444444      7.0\n",
      "is_LLDPE     0.875000  0.875000  0.875000      8.0\n",
      "is_PP        0.833333  1.000000  0.909091      5.0\n",
      "has_BSA      0.625000  0.666667  0.645161     15.0\n",
      "has_OIL      0.916667  0.916667  0.916667     12.0\n",
      "has_GUAR     1.000000  0.416667  0.588235     12.0\n",
      "has_CMC      0.705882  0.750000  0.727273     16.0\n",
      "has_STARCH   0.636364  0.583333  0.608696     12.0\n",
      "-- Classification report for target: is_HDPE --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.95      0.95        20\n",
      "         1.0       0.80      0.80      0.80         5\n",
      "\n",
      "    accuracy                           0.92        25\n",
      "   macro avg       0.88      0.88      0.88        25\n",
      "weighted avg       0.92      0.92      0.92        25\n",
      "\n",
      "-- Classification report for target: is_LDPE --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      1.00      0.88        18\n",
      "         1.0       1.00      0.29      0.44         7\n",
      "\n",
      "    accuracy                           0.80        25\n",
      "   macro avg       0.89      0.64      0.66        25\n",
      "weighted avg       0.84      0.80      0.76        25\n",
      "\n",
      "-- Classification report for target: is_LLDPE --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94        17\n",
      "         1.0       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.92        25\n",
      "   macro avg       0.91      0.91      0.91        25\n",
      "weighted avg       0.92      0.92      0.92        25\n",
      "\n",
      "-- Classification report for target: is_PP --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.97        20\n",
      "         1.0       0.83      1.00      0.91         5\n",
      "\n",
      "    accuracy                           0.96        25\n",
      "   macro avg       0.92      0.97      0.94        25\n",
      "weighted avg       0.97      0.96      0.96        25\n",
      "\n",
      "-- Classification report for target: has_BSA --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.44      0.40      0.42        10\n",
      "         1.0       0.62      0.67      0.65        15\n",
      "\n",
      "    accuracy                           0.56        25\n",
      "   macro avg       0.53      0.53      0.53        25\n",
      "weighted avg       0.55      0.56      0.56        25\n",
      "\n",
      "-- Classification report for target: has_OIL --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.92      0.92        13\n",
      "         1.0       0.92      0.92      0.92        12\n",
      "\n",
      "    accuracy                           0.92        25\n",
      "   macro avg       0.92      0.92      0.92        25\n",
      "weighted avg       0.92      0.92      0.92        25\n",
      "\n",
      "-- Classification report for target: has_GUAR --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      1.00      0.79        13\n",
      "         1.0       1.00      0.42      0.59        12\n",
      "\n",
      "    accuracy                           0.72        25\n",
      "   macro avg       0.82      0.71      0.69        25\n",
      "weighted avg       0.82      0.72      0.69        25\n",
      "\n",
      "-- Classification report for target: has_CMC --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.44      0.47         9\n",
      "         1.0       0.71      0.75      0.73        16\n",
      "\n",
      "    accuracy                           0.64        25\n",
      "   macro avg       0.60      0.60      0.60        25\n",
      "weighted avg       0.63      0.64      0.63        25\n",
      "\n",
      "-- Classification report for target: has_STARCH --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.69      0.67        13\n",
      "         1.0       0.64      0.58      0.61        12\n",
      "\n",
      "    accuracy                           0.64        25\n",
      "   macro avg       0.64      0.64      0.64        25\n",
      "weighted avg       0.64      0.64      0.64        25\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.64      0.69      0.67        13\n",
      "         1.0       0.64      0.58      0.61        12\n",
      "\n",
      "    accuracy                           0.64        25\n",
      "   macro avg       0.64      0.64      0.64        25\n",
      "weighted avg       0.64      0.64      0.64        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "metrics = {}\n",
    "# Exact-match accuracy (all targets correct)\n",
    "metrics['exact_match_accuracy'] = float(accuracy_score(y_test, y_pred))\n",
    "# Hamming loss (fraction of incorrect labels)\n",
    "metrics['hamming_loss'] = float(hamming_loss(y_test, y_pred))\n",
    "# Macro F1 across all binary targets\n",
    "try:\n",
    "    metrics['f1_macro'] = float(f1_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "except Exception as e:\n",
    "    metrics['f1_macro'] = None\n",
    "\n",
    "print('Exact-match accuracy:', metrics['exact_match_accuracy'])\n",
    "print('Hamming loss:', metrics['hamming_loss'])\n",
    "print('Macro F1:', metrics['f1_macro'])\n",
    "\n",
    "# Per-target precision/recall/f1/ support\n",
    "per_target = {}\n",
    "for i, tname in enumerate(target_names):\n",
    "    y_true_i = y_test[:, i]\n",
    "    y_pred_i = y_pred[:, i]\n",
    "    p, r, f, s = precision_recall_fscore_support(y_true_i, y_pred_i, average='binary', zero_division=0)\n",
    "    # support may be None in some sklearn versions/edge cases - compute from true labels as fallback\n",
    "    support_count = int(np.sum(y_true_i)) if y_true_i is not None else 0\n",
    "    per_target[tname] = {'precision': float(p), 'recall': float(r), 'f1': float(f), 'support': int(support_count)}\n",
    "\n",
    "metrics['per_target'] = per_target\n",
    "\n",
    "# Print a short per-target table\n",
    "import pandas as pd\n",
    "tbl = pd.DataFrame(per_target).T\n",
    "print('\\nPer-target metrics:')\n",
    "print(tbl)\n",
    "\n",
    "# Detailed classification reports per target (binary)\n",
    "for i, tname in enumerate(target_names):\n",
    "    print(f'-- Classification report for target: {tname} --')\n",
    "    try:\n",
    "        print(classification_report(y_test[:, i], y_pred[:, i], zero_division=0))\n",
    "    except Exception as e:\n",
    "        print('Could not produce classification_report for', tname, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e621281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to c:\\Users\\Mikey\\Documents\\GitHub\\cmse492_project\\src\\models\\baseline_multioutput_rf.joblib\n",
      "Saved metrics to c:\\Users\\Mikey\\Documents\\GitHub\\cmse492_project\\src\\models\\baseline_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Save model and metrics to disk under src/models/\n",
    "out_dir = repo_root / 'src' / 'models'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_path = out_dir / 'baseline_multioutput_rf.joblib'\n",
    "metrics_path = out_dir / 'baseline_metrics.json'\n",
    "joblib.dump(clf, model_path)\n",
    "with open(metrics_path, 'w', encoding='utf8') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print('Saved model to', model_path)\n",
    "print('Saved metrics to', metrics_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d705a73",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Consider cross-validation and hyperparameter search (GridSearchCV) for the RandomForest.\n",
    "- Try simpler linear models or tree-based feature importance to inspect which wavelengths matter.\n",
    "- Add a small unit test that asserts `baseline_multioutput_rf.joblib` and `baseline_metrics.json` exist after running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "368be173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation workbook to c:\\Users\\Mikey\\Documents\\GitHub\\cmse492_project\\src\\models\\baseline_metrics.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save evaluation results to an Excel workbook (summary + per-target)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "out_dir = repo_root / 'src' / 'models'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "xlsx_path = out_dir / 'baseline_metrics.xlsx'\n",
    "\n",
    "# Prepare summary dataframe\n",
    "summary = {\n",
    "    'exact_match_accuracy': metrics.get('exact_match_accuracy'),\n",
    "    'hamming_loss': metrics.get('hamming_loss'),\n",
    "    'f1_macro': metrics.get('f1_macro')\n",
    "}\n",
    "summary_df = pd.DataFrame([summary])\n",
    "\n",
    "# Per-target dataframe: prefer the `tbl` DataFrame if present, else construct from metrics\n",
    "if 'per_target' in metrics and metrics['per_target']:\n",
    "    per_target_df = pd.DataFrame(metrics['per_target']).T\n",
    "else:\n",
    "    try:\n",
    "        per_target_df = tbl.copy()\n",
    "    except NameError:\n",
    "        per_target_df = pd.DataFrame()\n",
    "\n",
    "# Write to Excel with two sheets\n",
    "try:\n",
    "    with pd.ExcelWriter(xlsx_path, engine='openpyxl') as writer:\n",
    "        summary_df.to_excel(writer, sheet_name='summary', index=False)\n",
    "        per_target_df.to_excel(writer, sheet_name='per_target')\n",
    "    print('Saved evaluation workbook to', xlsx_path)\n",
    "except Exception as e:\n",
    "    # fallback: try without specifying engine\n",
    "    with pd.ExcelWriter(xlsx_path) as writer:\n",
    "        summary_df.to_excel(writer, sheet_name='summary', index=False)\n",
    "        per_target_df.to_excel(writer, sheet_name='per_target')\n",
    "    print('Saved evaluation workbook to', xlsx_path, '(fallback writer used)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMSE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
